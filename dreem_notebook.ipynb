{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine Learning et classification   **     \n",
    "**Option Mathématiques appliquées 2019-2020  **    \n",
    "** Dreem 2 Sleep Stage Classification Challenge  **    \n",
    "    \n",
    "Antoine Guiot / Pierre Habin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Introduction**    \n",
    "Dreem est une société française fondée par deux anciens de l’École polytechnique et d'Isa Supaéro. Cette entreprise, à fort caractère technologique, développe un produit permettant d’améliorer la qualité du sommeil de leurs clients.\n",
    "En effet, les troubles du sommeil sont de plus en plus présents dans notre société (62% des Français rencontrent au moins un trouble du sommeil). Les méthodes conventionnelles d’analyses du sommeil comme la polysomnographie présentent certains désavantages : elles demandent notamment des équipements très importants et ne peuvent pas être réalisées dans l’environnement de sommeil classique du patient. Elles peuvent donc troubler le sommeil du patient et ainsi fausse les résultats. C’est pour pallier à cela que dreem a conçu un bandeau permettant à lui seul de réaliser cette analyse.\n",
    "\n",
    "Le bandeau dreem recueille ainsi de nombreuses données électroencéphalographiques qui correspondent à des signaux électriques traduisant l’activité du cerveau. Nous avons à disposition les données recueillies par le dispositif sur plusieurs nuits. L'enjeu de ce projet intervient dans le traitement de ces données.\n",
    "Effectivement, le bandeau doit dans un premier temps être capable de déterminer dans quelle phase de sommeil se situe le patient pour ensuite agir sur ses différentes phases. Nous allons donc appliquer des techniques de machine Learning et de classification pour relier les données du dispositif à l’une des 5 classes du sommeil.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Environement de travail**   \n",
    "Nous avons décidé de travailler sur l'environnement proposé par Kaggle dans le langage python.   \n",
    "Voici les librairies python que nous avons utilisées :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    " #This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "from sklearn.ensemble import RandomForestClassifier as RFC #Random Forest\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from sklearn.metrics import f1_score #F1score\n",
    "import tensorflow as tf \n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.utils import to_categorical\n",
    "#from entropy import spectral_entropy\n",
    "import seaborn\n",
    "from scipy.signal import periodogram, welch\n",
    "from scipy.stats import kurtosis, skew\n",
    "from scipy.signal import butter, lfilter\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "import h5py #file processing\n",
    "import matplotlib.pyplot as plt #graphics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Les données**    \n",
    "On dispose d'une base de données d'entrainement et d'une base de données de test. Ces bases contiennent des enregistrements de 30 secondes des signaux suivants :\n",
    "* 7 signaux issus des électroencéphalogrammes échantillonnés à 50 Hz\n",
    "* 3 signaux issus d'accéléromètre pour analyser les mouvements des patients\n",
    "* 1 signal issu d'un pulsomètre qui permet de déterminer le rythme cardiaque du patient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Chargement des données**   \n",
    "Nous allons dans un premier temps chargé les données de la base d'entrainement. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = \"/kaggle/input/dreem-sleep-stages-2020/X_train.h5\"\n",
    "\n",
    "train_set = h5py.File(filename,'r')\n",
    "y = pd.read_csv('/kaggle/input/dreem-sleep-stages-2020/y_train.csv')['sleep_stage']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous avons alors définit la liste des features de bases correspondants aux données bruts des signaux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "input_feature_list = \n",
    "['eeg_1',\n",
    " 'eeg_2',\n",
    " 'eeg_3',\n",
    " 'eeg_4',\n",
    " 'eeg_5',\n",
    " 'eeg_6',\n",
    " 'eeg_7',\n",
    " 'x',\n",
    " 'y',\n",
    " 'z',\n",
    " 'pulse']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici un exemple de signal brut receuilli par le dispositif "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 232\n",
    "index_1 = y[y==4].index.values\n",
    "plt.plot(train_set['eeg_6'][index_1[i]],label='eeg_1')\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque alors que le signal est trés bruité. Nous avons donc décidé d'effectuer une étape de prétraitement des données brutes avant l'application des techniques de machine learning et de classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prétraitement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Idée\n",
    "   \n",
    "Pour réduire le bruit nous avons voulu filtrer les signaux d'entrées avec un filtre passe bande de Butterworth, pour cela nous nous sommes intéréssé aux fréquences caractéristiques des ondes cérébrales associées au sommeil."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Fréquence caractéristique du cerveau**   \n",
    "Les données issues des électroencéphalogrammes présentent une décomposition en plusieurs bandes de fréquences. Certaines de ces bandes de fréquences sont étroitement liées avec le sommeil : \n",
    "*  La bande Delta entre 0 et 4 Hz, associé au sommeil profond. \n",
    "*  Les ondes Thêta entre 4 et 7,5 Hz, ces ondes apparaissent lorsque le sujet est un état de somnolence.\n",
    "*  Les ondes Alpha entre 8 et 12 Hz, ces ondes  caractérisent un état de conscience apaisé, et sont principalement émises lorsque le sujet a les yeux fermés.\n",
    "*  Les ondes Fuseaux, des ondes comprises entre 12 et 16 Hz, caractéristiques de la phase de sommeil léger.   \n",
    "   \n",
    "Nous allons donc dimensioner nos filtres de Butterworth dans des zones proches de ces fréquences liées au sommeil. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Filtre de Butterworth**   \n",
    "\n",
    "Nous avons alors construit des fonctions permettant de réaliser des filtres de Butterworth à appliquer à nos données. Voici les fonctions correspondantes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    result=dict()\n",
    "    new_feature_list = []\n",
    "    for item in input_feature_list:\n",
    "        b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "        result[item+'_'+ str(highcut)] = lfilter(b, a, data[item])\n",
    "        new_feature_list.append(item+'_'+ str(highcut))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ces fonctions nous permettent de choisir les bandes passantes que nous souhaitons appliquer aux données. Nous utilisons des filtres d'ordre 5 par défaut.   \n",
    "Nous pouvons donc maintenant appliquer ces filtres à nos données en ciblant les bandes passantes fréquentielles explicités precedement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_filtered_4 = butter_bandpass_filter(train_set, 0.5, 4,50)\n",
    "data_filtered_8 = butter_bandpass_filter(train_set, 4, 8,50)\n",
    "data_filtered_13 =butter_bandpass_filter(train_set, 8, 13,50)\n",
    "data_filtered_22 = butter_bandpass_filter(train_set, 13, 22,50)\n",
    "data_filtered_25 = butter_bandpass_filter(train_set, 22, 24.99,50)\n",
    "\n",
    "input_data = dict (data_filtered_25, **data_filtered_22, **data_filtered_13, **data_filtered_8, **data_filtered_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features   \n",
    "Pour appliquer les algorythmes de classifications nous avions dans un premier temps besoin de définir des features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition des features\n",
    "\n",
    "Nous avons décidé de calculer pour chaque signal :\n",
    "* Le maximum\n",
    "* Le minimum\n",
    "* La moyenne\n",
    "* Le zero-crossing\n",
    "* L'ecart-type\n",
    "* L'entropie\n",
    "* Le kurtosis\n",
    "* L'assymétrie\n",
    "\n",
    "Voici les fonctions permettant de creer ces features :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_max_feature(data,df,feature_list,signals_list):\n",
    "    for item in signals_list:\n",
    "        feature_list.append(item+'_max')\n",
    "        df[item+'_max']=np.max(data[item],axis=1)\n",
    "    print('max feature : done')\n",
    "    return df\n",
    "\n",
    "def create_min_feature(data,df,feature_list,signals_list):\n",
    "    for item in signals_list:\n",
    "        feature_list.append(item+'_min')\n",
    "        df[item+'_min']=np.min(data[item],axis=1)\n",
    "    print('min feature : done')\n",
    "    return df\n",
    "\n",
    "def create_mean_feature(data,df,feature_list,signals_list):\n",
    "    for item in signals_list:\n",
    "        feature_list.append(item+'_mean')\n",
    "        df[item+'_mean']=np.mean(data[item],axis=1)\n",
    "    print('mean feature : done')\n",
    "    return df\n",
    "\n",
    "def create_zero_crossing(data,df,feature_list,signals_list):\n",
    "    for item in signals_list:\n",
    "        feature_list.append(item+'_zeros_crossing')\n",
    "        df[item+'_zero_crossing']= np.sum(np.diff(np.signbit(data[item])),axis=1)\n",
    "    print('zero crossing feature : done')\n",
    "    return df\n",
    "\n",
    "def create_deviation_feature(data,df,feature_list,signals_list):\n",
    "    for item in signals_list:\n",
    "        feature_list.append(item+'_deviation')\n",
    "        df[item+'_deviation']=np.std(data[item],axis=1)\n",
    "    print('deviation feature : done')\n",
    "    return df\n",
    "\n",
    "def create_spectral_entropy(data,df,feature_list,signals_list):\n",
    "    for item in signals_list:\n",
    "        feature_list.append(item+'_spec_entropy')\n",
    "        #x = np.array(x)\n",
    "    # Compute and normalize power spectrum\n",
    "        _, psd = periodogram(data[item],axis=1)\n",
    "        psd_norm = psd/ psd.sum(axis=1).reshape((len(psd),1))\n",
    "        df[item+'_spec_entropy'] = -np.multiply(psd_norm, np.nan_to_num(np.log2(psd_norm))).sum(axis=1)\n",
    "    print('spectrale entropy feature : done')\n",
    "    return df\n",
    "\n",
    "def create_kurtosis(data,df,feature_list,signals_list):\n",
    "    for item in signals_list:\n",
    "        feature_list.append(item+'_kurtosis')\n",
    "        df[item+'_kurtosis']=kurtosis(data[item],axis=1)\n",
    "    print('kurtosis feature : done')\n",
    "    return df\n",
    "\n",
    "def create_skew(data,df,feature_list,signals_list):\n",
    "    for item in signals_list:\n",
    "        feature_list.append(item+'_skew')\n",
    "        df[item+'_skew']=skew(data[item],axis=1)\n",
    "    print('skew feature : done')\n",
    "    return df\n",
    "\n",
    "def hjorth(X, D = None): # does not work for now\n",
    "\n",
    "    if D is None:\n",
    "        D = first_order_diff(X)\n",
    "\n",
    "    D.insert(0, X[0]) # pad the first difference\n",
    "    D = NP;array(D)\n",
    "\n",
    "    n = len(X)\n",
    "\n",
    "    M2 = float(sum(D ** 2)) / n\n",
    "    TP = np.sum(X ** 2)\n",
    "    M4 = 0;\n",
    "    for i in xrange(1, len(D)):\n",
    "        M4 += (D[i] - D[i - 1]) ** 2\n",
    "    M4 = M4 / n\n",
    "\n",
    "    return sqrt(M2 / TP), sqrt(float(M4) * TP / M2 / M2)\n",
    "\n",
    "def create_target(data,df):\n",
    "    df['target']= pd.read_csv('/kaggle/input/dreem-sleep-stages-2020/y_train.csv')['sleep_stage'].values\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Application des features\n",
    "\n",
    "Nous avons ensuite appliquer ces fonctions à l'ensembles des signaux afin d'obtenir un dataframme exploitable pour la classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =pd.DataFrame()\n",
    "def feature_creation(data, is_training_set=True, signals_list=None ):\n",
    "    feature_list=[]\n",
    "    df =pd.DataFrame()\n",
    "    create_max_feature(data,df,feature_list, signals_list) #MAX\n",
    "    create_min_feature(data,df,feature_list, signals_list) #MIN\n",
    "    create_mean_feature(data,df,feature_list, signals_list) #MOYENNE\n",
    "    create_zero_crossing(data,df,feature_list, signals_list) #ZERO-CROSSING\n",
    "    create_deviation_feature(data,df,feature_list, signals_list) #ECART-TYPE\n",
    "    create_spectral_entropy(data,df,feature_list, signals_list) #ENTROPIE\n",
    "    create_kurtosis(data,df,feature_list, signals_list) #KURTOSIS\n",
    "    create_skew(data,df,feature_list,signals_list) #ASSYMETRIE\n",
    "    if is_training_set==True:\n",
    "        create_target(data,df)\n",
    "    return df, feature_list\n",
    "training_df , feature_list = feature_creation(input_data, is_training_set=True, signals_list=list(input_data.keys()))\n",
    "\n",
    "del input_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def scaler(data,)\n",
    "scaler = StandardScaler()\n",
    "training_df_scaled = scaler.fit_transform(training_df.loc[:, training_df.columns != 'target'].values)\n",
    "training_df_scaled=pd.DataFrame(training_df_scaled,columns=feature_list)\n",
    "training_df_scaled['target']=training_df['target'].values\n",
    "train_df, test_df = train_test_split(training_df_scaled, shuffle=True)\n",
    "del training_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrice de corrélations\n",
    "\n",
    "Nous pouvons calculer la matrice de corrélation associés à ces différentes features :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = train_df.corr().abs()\n",
    "plt.figure(figsize=(15,15))\n",
    "seaborn.heatmap(corr_matrix, annot=False, cmap=plt.cm.Reds)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour rendre cela plus lisible on peut tenter de regarder quels sont les maximums de corrélations entre les différentes features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = (corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n",
    "                 .stack()\n",
    "                 .sort_values(ascending=False))\n",
    "print(sol.iloc[0:25])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "De manière assez logique les plus fortes corrélations sont issus des liens entre les différents maximum et entre maximum - ecart type. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble de test  \n",
    "\n",
    "Nous allons étudier la répartitions des différentes classes dans l'ensemble de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys = [0,1,2,3,4]\n",
    "class_weight = dict(zip(keys, np.sum(to_categorical(train_df['target'].values),axis=0)/np.sum(to_categorical(train_df['target'].values))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On observe que la classe 1 est sous représentée, par conséquent elle sera probablement plus difficile à prédire avec précision. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest \n",
    "\n",
    "La méthode de classifiation que nous avons décidé d'utiliser est une méthode de type Random Forest. Nous pensons que l'application du prétraitement et que les features nous permettrons d'obtenir un score meilleur que le Simple Random Forest donné en exemple. \n",
    "\n",
    "\n",
    "### création du Random Forest\n",
    "\n",
    "Pour creer le random forest nous allons utiliser la librairie *RandomForestClassifier* de *sklearn*. On fixe la profondeur maximale à 50 et on utilise en entrée les class_weight dans le but d'améliorer la précision.\n",
    "On applique ensuite le random forest à nos données filtrées avec les features réalisé precedement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RFC(n_estimators=500, max_depth=50,\n",
    "                              random_state=0,class_weight=class_weight)\n",
    "clf.fit(train_df.loc[:, train_df.columns != 'target'].values,train_df['target'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance des features  \n",
    "Nous allons maintenant étudier l'importance de chaques features dans le processus de classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importances = clf.feature_importances_\n",
    "feature_list = train_df.columns.to_list()[0:len(train_df.columns.to_list())-1]\n",
    "feature_importance = dict(zip(feature_list,importances))\n",
    "Importance_list = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "Importance_list[0:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que les zeros crossing et les ecarts types sur les signaux filtrés des électroencéphalogrammes semblent être les varaibles les plus décisives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Résultats\n",
    "\n",
    "Nous allons maintenant étudier les résultats du Random Forest sur les données de test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(test_df.loc[:, test_df.columns != 'target'].values)\n",
    "y_true = test_df['target'].values\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous obtenons un F1 score global de l'odre de 80%. \n",
    "Nous pouvons également comparé les class_weight entre les données de test et les prédictions de notre modéle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_weight_predicted_val = dict(zip(keys, np.sum(to_categorical(y_pred),axis=0)/np.sum(to_categorical(y_pred))))\n",
    "print('predict :',class_weight_predicted_val)\n",
    "class_weight_true_val = dict(zip(keys, np.sum(to_categorical(y_true),axis=0)/np.sum(to_categorical(y_true))))\n",
    "print('true :',class_weight_true_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

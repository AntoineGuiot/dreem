{"cells":[{"metadata":{},"cell_type":"markdown","source":"**Machine Learning et classification   **     \n**Option Mathématiques appliquées 2019-2020  **    \n** Dreem 2 Sleep Stage Classification Challenge  **    \n    \nAntoine Guiot / Pierre Habin"},{"metadata":{},"cell_type":"markdown","source":"# **Introduction**    \nDreem est une société française fondée par deux anciens de l’école Polytechnique et d'ISA Supaéro. Cette entreprise, à fort caractère technologique, développe un produit permettant d’améliorer la qualité du sommeil de leurs clients.\nEn effet, les troubles du sommeil sont de plus en plus présents dans notre société (62% des Français rencontrent au moins un trouble du sommeil). Les méthodes conventionnelles d’analyses du sommeil comme la polysomnographie présentent certains désavantages : elles demandent notamment des équipements très importants et ne peuvent pas être réalisées dans l’environnement de sommeil classique du patient. Elles peuvent donc troubler le sommeil du patient et ainsi fausse les résultats. C’est pour pallier à cela que dreem a conçu un bandeau permettant à lui seul de réaliser cette analyse.\n\nLe bandeau dreem recueille ainsi de nombreuses données électroencéphalographiques qui correspondent à des signaux électriques traduisant l’activité du cerveau. Nous avons à disposition les données recueillies par le dispositif sur plusieurs nuits. L'enjeu de ce projet intervient dans le traitement de ces données.\nEffectivement, le bandeau doit dans un premier temps être capable de déterminer dans quelle phase de sommeil se situe le patient pour ensuite agir sur ses différentes phases. Nous allons donc appliquer des techniques de machine Learning et de classification pour relier les données du dispositif à l’une des 5 classes du sommeil.\n"},{"metadata":{},"cell_type":"markdown","source":"### Environnement de travail  \nNous avons décidé de travailler sur l'environnement proposé par Kaggle dans le langage python.\nVoici les librairies python que nous avons utilisées :"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":" #This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \nfrom sklearn.ensemble import RandomForestClassifier as RFC #Random Forest\nfrom sklearn.model_selection import train_test_split\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.metrics import f1_score #F1score\nimport tensorflow as tf \nfrom sklearn.preprocessing import StandardScaler\nfrom keras.utils import to_categorical\n#from entropy import spectral_entropy\nimport seaborn\nfrom scipy.signal import periodogram, welch\nfrom scipy.stats import kurtosis, skew\nfrom scipy.signal import butter, lfilter\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport h5py #file processing\nimport matplotlib.pyplot as plt #graphics","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### **Les données**    \nOn dispose d'une base de données d'entrainement et d'une base de données de test. Ces bases contiennent des enregistrements de 30 secondes des signaux suivants :\n* 7 signaux issus des électroencéphalogrammes échantillonnés à 50 Hz\n* 3 signaux issus d'accéléromètre pour analyser les mouvements des patients\n* 1 signal issu d'un pulsomètre qui permet de déterminer le rythme cardiaque du patient"},{"metadata":{},"cell_type":"markdown","source":"### Chargement des données   \nNous allons dans un premier temps chargé les données de la base d'entrainement. "},{"metadata":{"trusted":true},"cell_type":"code","source":"filename = \"/kaggle/input/dreem-sleep-stages-2020/X_train.h5\"\n\ntrain_set = h5py.File(filename,'r')\ny = pd.read_csv('/kaggle/input/dreem-sleep-stages-2020/y_train.csv')['sleep_stage']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nous avons alors défini la liste des features de bases correspondant aux données brutes des signaux"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"input_feature_list = ['eeg_1',\n 'eeg_2',\n 'eeg_3',\n 'eeg_4',\n 'eeg_5',\n 'eeg_6',\n 'eeg_7',\n 'x',\n 'y',\n 'z',\n 'pulse']","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Voici un exemple de signal brut recueilli par le dispositif"},{"metadata":{"trusted":true},"cell_type":"code","source":"i = 232\nindex_1 = y[y==4].index.values\nplt.plot(train_set['eeg_6'][index_1[i]],label='eeg_1')\n\nplt.legend()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On remarque alors que le signal est très bruité. Nous avons donc décidé d'effectuer une étape de prétraitement des données brutes avant l'application des techniques de machine learning et de classification."},{"metadata":{},"cell_type":"markdown","source":"# Prétraitement des données"},{"metadata":{},"cell_type":"markdown","source":"### Idée\n   \nPour réduire le bruit nous avons voulu filtrer les signaux d'entrées avec un filtre passe bande de Butterworth, pour cela nous nous sommes intéréssé aux fréquences caractéristiques des ondes cérébrales associées au sommeil."},{"metadata":{},"cell_type":"markdown","source":"### **Fréquence caractéristique du cerveau**   \nLes données issues des électroencéphalogrammes présentent une décomposition en plusieurs bandes de fréquences. Certaines de ces bandes de fréquences sont étroitement liées avec le sommeil : \n*  La bande Delta entre 0 et 4 Hz, associée au sommeil profond. \n*  Les ondes Thêta entre 4 et 7,5 Hz, ces ondes apparaissent lorsque le sujet est un état de somnolence.\n*  Les ondes Alpha entre 8 et 12 Hz, ces ondes  caractérisent un état de conscience apaisé, et sont principalement émises lorsque le sujet a les yeux fermés.\n*  Les ondes Fuseaux, des ondes comprises entre 12 et 16 Hz, caractéristiques de la phase de sommeil léger.   \n   \nNous allons donc dimensionner nos filtres de Butterworth dans des zones proches de ces fréquences liées au sommeil."},{"metadata":{},"cell_type":"markdown","source":"### **Filtre de Butterworth**   \n\nNous avons alors construit des fonctions permettant de réaliser des filtres de Butterworth à appliquer à nos données. Voici les fonctions correspondantes "},{"metadata":{"trusted":true},"cell_type":"code","source":"def butter_bandpass(lowcut, highcut, fs, order=5):\n    nyq = 0.5 * fs\n    low = lowcut / nyq\n    high = highcut / nyq\n    b, a = butter(order, [low, high], btype='band')\n    return b, a\n\ndef butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n    result=dict()\n    new_feature_list = []\n    for item in input_feature_list:\n        b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n        result[item+'_'+ str(highcut)] = lfilter(b, a, data[item])\n        new_feature_list.append(item+'_'+ str(highcut))\n    return result","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Ces fonctions nous permettent de choisir les bandes passantes que nous souhaitons appliquer aux données. Nous utilisons des filtres d'ordre 5 par défaut.\nNous pouvons donc maintenant appliquer ces filtres à nos données en ciblant les bandes passantes fréquentielles explicités précédemment."},{"metadata":{"trusted":true},"cell_type":"code","source":"data_filtered_4 = butter_bandpass_filter(train_set, 0.5, 4,50)\ndata_filtered_8 = butter_bandpass_filter(train_set, 4, 8,50)\ndata_filtered_13 =butter_bandpass_filter(train_set, 8, 13,50)\ndata_filtered_22 = butter_bandpass_filter(train_set, 13, 22,50)\ndata_filtered_25 = butter_bandpass_filter(train_set, 22, 24.99,50)\n\ninput_data = dict (data_filtered_25, **data_filtered_22, **data_filtered_13, **data_filtered_8, **data_filtered_4)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Features    \n\nPour appliquer les algorithmes de classifications nous avions dans un premier temps besoin de définir des features."},{"metadata":{},"cell_type":"markdown","source":"### Définition des features\n\nNous avons décidé de calculer pour chaque signal :\n* Le maximum\n* Le minimum\n* La moyenne\n* Le zero-crossing\n* L'ecart-type\n* L'entropie\n* Le kurtosis\n* L'assymétrie\n\nVoici les fonctions permettant de créer ces features :"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_max_feature(data,df,feature_list,signals_list):\n    for item in signals_list:\n        feature_list.append(item+'_max')\n        df[item+'_max']=np.max(data[item],axis=1)\n    print('max feature : done')\n    return df\n\ndef create_min_feature(data,df,feature_list,signals_list):\n    for item in signals_list:\n        feature_list.append(item+'_min')\n        df[item+'_min']=np.min(data[item],axis=1)\n    print('min feature : done')\n    return df\n\ndef create_mean_feature(data,df,feature_list,signals_list):\n    for item in signals_list:\n        feature_list.append(item+'_mean')\n        df[item+'_mean']=np.mean(data[item],axis=1)\n    print('mean feature : done')\n    return df\n\ndef create_zero_crossing(data,df,feature_list,signals_list):\n    for item in signals_list:\n        feature_list.append(item+'_zeros_crossing')\n        df[item+'_zero_crossing']= np.sum(np.diff(np.signbit(data[item])),axis=1)\n    print('zero crossing feature : done')\n    return df\n\ndef create_deviation_feature(data,df,feature_list,signals_list):\n    for item in signals_list:\n        feature_list.append(item+'_deviation')\n        df[item+'_deviation']=np.std(data[item],axis=1)\n    print('deviation feature : done')\n    return df\n\ndef create_spectral_entropy(data,df,feature_list,signals_list):\n    for item in signals_list:\n        feature_list.append(item+'_spec_entropy')\n        #x = np.array(x)\n    # Compute and normalize power spectrum\n        _, psd = periodogram(data[item],axis=1)\n        psd_norm = psd/ psd.sum(axis=1).reshape((len(psd),1))\n        df[item+'_spec_entropy'] = -np.multiply(psd_norm, np.nan_to_num(np.log2(psd_norm))).sum(axis=1)\n    print('spectrale entropy feature : done')\n    return df\n\ndef create_kurtosis(data,df,feature_list,signals_list):\n    for item in signals_list:\n        feature_list.append(item+'_kurtosis')\n        df[item+'_kurtosis']=kurtosis(data[item],axis=1)\n    print('kurtosis feature : done')\n    return df\n\ndef create_skew(data,df,feature_list,signals_list):\n    for item in signals_list:\n        feature_list.append(item+'_skew')\n        df[item+'_skew']=skew(data[item],axis=1)\n    print('skew feature : done')\n    return df\n\ndef hjorth(X, D = None): # does not work for now\n\n    if D is None:\n        D = first_order_diff(X)\n\n    D.insert(0, X[0]) # pad the first difference\n    D = NP;array(D)\n\n    n = len(X)\n\n    M2 = float(sum(D ** 2)) / n\n    TP = np.sum(X ** 2)\n    M4 = 0;\n    for i in xrange(1, len(D)):\n        M4 += (D[i] - D[i - 1]) ** 2\n    M4 = M4 / n\n\n    return sqrt(M2 / TP), sqrt(float(M4) * TP / M2 / M2)\n\ndef create_target(data,df):\n    df['target']= pd.read_csv('/kaggle/input/dreem-sleep-stages-2020/y_train.csv')['sleep_stage'].values\n    return df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Application des features\n\nNous avons ensuite appliqué ces fonctions à l'ensemble des signaux afin d'obtenir un datagramme exploitable pour la classification."},{"metadata":{"trusted":true},"cell_type":"code","source":"df =pd.DataFrame()\ndef feature_creation(data, is_training_set=True, signals_list=None ):\n    feature_list=[]\n    df =pd.DataFrame()\n    create_max_feature(data,df,feature_list, signals_list) #MAX\n    create_min_feature(data,df,feature_list, signals_list) #MIN\n    create_mean_feature(data,df,feature_list, signals_list) #MOYENNE\n    create_zero_crossing(data,df,feature_list, signals_list) #ZERO-CROSSING\n    create_deviation_feature(data,df,feature_list, signals_list) #ECART-TYPE\n    create_spectral_entropy(data,df,feature_list, signals_list) #ENTROPIE\n    create_kurtosis(data,df,feature_list, signals_list) #KURTOSIS\n    create_skew(data,df,feature_list,signals_list) #ASSYMETRIE\n    if is_training_set==True:\n        create_target(data,df)\n    return df, feature_list\ntraining_df , feature_list = feature_creation(input_data, is_training_set=True, signals_list=list(input_data.keys()))\n\ndel input_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"scaler"},{"metadata":{"trusted":true},"cell_type":"code","source":"#def scaler(data,)\nscaler = StandardScaler()\ntraining_df_scaled = scaler.fit_transform(training_df.loc[:, training_df.columns != 'target'].values)\ntraining_df_scaled=pd.DataFrame(training_df_scaled,columns=feature_list)\ntraining_df_scaled['target']=training_df['target'].values\ntrain_df, test_df = train_test_split(training_df_scaled, shuffle=True)\ndel training_df","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Matrice de corrélations\n\nNous pouvons calculer la matrice de corrélation associée à ces différentes features ;"},{"metadata":{"trusted":true},"cell_type":"code","source":"corr_matrix = train_df.corr().abs()\nplt.figure(figsize=(15,15))\nseaborn.heatmap(corr_matrix, annot=False, cmap=plt.cm.Reds)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Pour rendre cela plus lisible on peut tenter de regarder quels sont les maximums de corrélations entre les différentes features."},{"metadata":{"trusted":true},"cell_type":"code","source":"sol = (corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))\n                 .stack()\n                 .sort_values(ascending=False))\nprint(sol.iloc[0:25])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"De manière assez logique les plus fortes corrélations sont issues des liens entre les différents maximums et entre maximums - écart-type."},{"metadata":{},"cell_type":"markdown","source":"# Ensemble de test  \n\nNous allons étudier la répartition des différentes classes dans l'ensemble de test."},{"metadata":{"trusted":true},"cell_type":"code","source":"keys = [0,1,2,3,4]\nclass_weight = dict(zip(keys, np.sum(to_categorical(train_df['target'].values),axis=0)/np.sum(to_categorical(train_df['target'].values))))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_weight","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On observe que la classe 1 est sous représentée, par conséquent elle sera probablement plus difficile à prédire avec précision. "},{"metadata":{},"cell_type":"markdown","source":"# Random Forest \n\nLa méthode de classification que nous avons décidé d'utiliser est une méthode du type Random Forest. Nous pensons que l'application du prétraitement et que les features nous permettront d'obtenir un score meilleur que le Simple random Forest donné en exemple.\n\n\n### création du Random Forest\n\nPour creer le random forest nous allons utiliser la librairie *RandomForestClassifier de *sklearn*. On fixe la profondeur maximale à 50 et on utilise en entrée les class weight dans le but d'améliorer la précision.   \nOn applique ensuite le random forest à nos données filtrées avec les features réalisés précédemment."},{"metadata":{"trusted":true},"cell_type":"code","source":"clf = RFC(n_estimators=500, max_depth=50,\n                              random_state=0,class_weight=class_weight)\nclf.fit(train_df.loc[:, train_df.columns != 'target'].values,train_df['target'])  ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Importance des features    \n   \nNous allons maintenant étudier l'importance de chaque feature dans le processus de classification."},{"metadata":{"trusted":true},"cell_type":"code","source":"importances = clf.feature_importances_\nfeature_list = train_df.columns.to_list()[0:len(train_df.columns.to_list())-1]\nfeature_importance = dict(zip(feature_list,importances))\nImportance_list = sorted(feature_importance.items(), key=lambda x: x[1], reverse=True)\nImportance_list[0:25]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"On remarque que les zéros crossing et les écarts-types sur les signaux filtrés des électroencéphalogrammes semblent être les variables les plus décisives."},{"metadata":{},"cell_type":"markdown","source":"# Résultats\n\n\n### Calcul du score \n\nLe score de notre algorithme  est calculé sous la forme d'une moyenne de F1 score. Le F1 score correspond à la valeur : $ 2 \\frac{p.r}{p+r} $ ou $p$ représente la précision et $r$ le recall.    \nla précision correspond au ratio de vrai positif sur le nombre prédit de positif et le recall correspond au ratio de vrai positif sur le nombre total de positif dans l'échantillon. Un bon algorithme doit assurer une bonne valeur de ces deux indicateurs.   \nIci on calcule le F1 score pour chacune des classes puis on en prends la moyenne pondérée par la fréquence d'apparition de chaque classe. \n\n\n"},{"metadata":{},"cell_type":"markdown","source":"### Score sur l'échantillon d'apprentissage\n\nNous allons maintenant étudier les résultats du Random Forest sur les données de test. "},{"metadata":{"trusted":true},"cell_type":"code","source":"y_pred = clf.predict(test_df.loc[:, test_df.columns != 'target'].values)\ny_true = test_df['target'].values\nfrom sklearn.metrics import classification_report\nprint(classification_report(y_true, y_pred))\nprint(y_pred)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Nous obtenons un F1 score global de l'odre de 0,80.   \nNous pouvons également comparé les class_weight entre les données de test et les prédictions de notre modéle."},{"metadata":{"trusted":true},"cell_type":"code","source":"class_weight_predicted_val = dict(zip(keys, np.sum(to_categorical(y_pred),axis=0)/np.sum(to_categorical(y_pred))))\nprint('predict :',class_weight_predicted_val)\nclass_weight_true_val = dict(zip(keys, np.sum(to_categorical(y_true),axis=0)/np.sum(to_categorical(y_true))))\nprint('true :',class_weight_true_val)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Score sur l'échantillon de test\n\nNous pouvons maintenant procéder au test de notre algorithme sur l'échantillon de test.   \nAprès soumission de notre algorithme nous obtenons un score final de 0,71. Ce score est inférieur à celui obtenu sur l'échantillon de test, cela peut provenir de notre manque de précision sur la classe 1 qui est surement plus représentée dans l'échantillon de test que dans celui d'apprentissage."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}